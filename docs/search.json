[
  {
    "objectID": "open_model.html",
    "href": "open_model.html",
    "title": "Baseline model for predicting disease outbreak periods",
    "section": "",
    "text": "Overview\nWe created an outbreak prediction baseline model that demonstrates the use of the Project CCHAIN linked dataset in a machine learning use case.\n\n\n\n\n\n\nObjective: Predict dengue outbreaks and the number of cases for Zamboanga City at the weekly scale\n\n\n\nThe following must be included as features for the model:\n\n\n\n\n\n\n\nFactor\nVariables\n\n\n\n\nClimate\ntemperature, humidity, rainfall, solar radiation, wind speed, vegetation, air quality\n\n\nDemographics\npopulation, population density\n\n\nFacilities\nclinics, hospitals, sanitation, water source,\n\n\nEconomic activity\nnightlights radiance\n\n\n\nFor this model, we will define the training and testing set as follows:\n\n\n\nSet\nTime covered\n\n\n\n\nTraining\nJan 2014-Mar 2019 (75 mos)\n\n\nTesting\nApr 2014-Dec 2020 (20 mos)\n\n\n\n\n\n\n\n\n\nAccess the full model notebook here: GDrive, Github\n\n\n\n\n\nResults"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quick links",
    "section": "",
    "text": "Linking Village-Level Data on Climatic Impact-Drivers, Socioeconomic Vulnerability, and Environmental Conditions to Health Impacts across 12 Philippine Cities\n  The Project CCHAIN dataset is a validated, open-sourced linked dataset measuring 20 years (2003-2022) of climate, environmental, socioeconomic, and health variables at the barangay (village) level across 12 Philippine cities.\n\n\nQuick links\n\nAccess the data here:\n\nGoogle Drive\nKaggle\nHDX\n\nSee full data documentation on here\nTo access the informal settlement community survey data, send your request for approval here\n\n\n\nProject Team\nConsortium members\n\n\n\n\n\n\n\n\n\nThinking Machines\n\n\n\n\n\n\n\nEpimetrics\n\n\n\n\n\n\n\nManila Observatory\n\n\n\n\n\n\n\nPACSII\n\n\n\n\n\nSupported by\n\n\n\n\n\n\n\n\n\nLacuna Fund\n\n\n\n\n\n\n\nWellcome Trust\n\n\n\n\n\n\n\nContact Us\nFor general project inquiries, send a message to data-for-development@thinkingmachin.es"
  },
  {
    "objectID": "notebooks/baseline-model/baseline-model-demo.html",
    "href": "notebooks/baseline-model/baseline-model-demo.html",
    "title": "Using the linked dataset in a baseline model for predicting disease outbreak periods",
    "section": "",
    "text": "We created an outbreak prediction baseline model that demonstrates the use of the Project CCHAIN linked dataset in a machine learning use case.\nObjective: Predict dengue outbreaks and the number of cases for Zamboanga City at the weekly scale\nThe following must be included as features for the model:\nFor this model, we will define the training and testing set as follows:\nimport sys\n\nsys.path.append(\"../../\")\nfrom src.model_data_prep import create_outbreak_summary\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import (\n    r2_score,\n    mean_absolute_error,\n    mean_squared_error,\n    confusion_matrix,\n    classification_report,\n    ConfusionMatrixDisplay,\n)\nimport shap\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nPROCESSED_DIR = Path(\"../../data/03-processed/\")\n# LINKED_DATA = PROCESSED_DIR / \"linked_df_city_weekly_dengue.csv\"\nLINKED_DATA = (\n    PROCESSED_DIR / \"linked_training_data/linked_df_pidsr_city_weekly_dengue.csv\"\n)"
  },
  {
    "objectID": "notebooks/baseline-model/baseline-model-demo.html#load-the-labeled-linked-dataset",
    "href": "notebooks/baseline-model/baseline-model-demo.html#load-the-labeled-linked-dataset",
    "title": "Using the linked dataset in a baseline model for predicting disease outbreak periods",
    "section": "Load the labeled linked dataset",
    "text": "Load the labeled linked dataset\nGenerate the linked dataset using the Linking Colaboratory Notebook and download locally. Specifications on the temporal coverage, tables, and columns needed are listed in the summary at the start of this notebook.\n\nlinked_data = pd.read_csv(LINKED_DATA)\nlinked_data.head()\n\n\n\n\n\n\n\n\n\nSource\nyear\nMonth\nWeek\nDate\nRegion\nPSGC_Region\nMunicipality\nADM3_PCODE\nICD\n...\ngoogle_bldgs_density_mean\ngoogle_bldgs_pct_built_up_area_sum\ngoogle_bldgs_pct_built_up_area_mean\ntravel_time\nhospital_pop_reached_total\nhospital_pct_population_reached\nhealthcenter_pop_reached_total\nhealthcenter_pct_population_reached\nrhu_pop_reached_total\nrhu_pct_population_reached\n\n\n\n\n0\nPIDSR-DOH\n2008.0\n1.0\n1\n2008-01-07\nRegion IX\nPH090000000\nZamboanga City\nPH097332000\nA90-A91\n...\n0.001116\n908.037478\n8.99047\n5\n124434.91\n14.46\n456862.05\n53.09\n301674.11\n35.06\n\n\n1\nPIDSR-DOH\n2008.0\n1.0\n2\n2008-01-07\nRegion IX\nPH090000000\nZamboanga City\nPH097332000\nA90-A91\n...\n0.001116\n908.037478\n8.99047\n5\n124434.91\n14.46\n456862.05\n53.09\n301674.11\n35.06\n\n\n2\nPIDSR-DOH\n2008.0\n1.0\n3\n2008-01-14\nRegion IX\nPH090000000\nZamboanga City\nPH097332000\nA90-A91\n...\n0.001116\n908.037478\n8.99047\n5\n124434.91\n14.46\n456862.05\n53.09\n301674.11\n35.06\n\n\n3\nPIDSR-DOH\n2008.0\n1.0\n4\n2008-01-21\nRegion IX\nPH090000000\nZamboanga City\nPH097332000\nA90-A91\n...\n0.001116\n908.037478\n8.99047\n5\n124434.91\n14.46\n456862.05\n53.09\n301674.11\n35.06\n\n\n4\nPIDSR-DOH\n2008.0\n1.0\n5\n2008-01-28\nRegion IX\nPH090000000\nZamboanga City\nPH097332000\nA90-A91\n...\n0.001116\n908.037478\n8.99047\n5\n124434.91\n14.46\n456862.05\n53.09\n301674.11\n35.06\n\n\n\n\n5 rows × 255 columns\n\n\n\n\n\nRemove once we replace with linked data from linking notebook\n\nSort the dataframe by Date and place it at the left-most column.\n\nlinked_data[\"Date\"] = pd.to_datetime(linked_data[\"Date\"])\nlinked_data = linked_data.sort_values(by=[\"Date\"])\nlinked_data = linked_data.drop(columns=[\"start_of_week\"])\nlinked_data.insert(0, \"Date\", linked_data.pop(\"Date\"))"
  },
  {
    "objectID": "notebooks/baseline-model/baseline-model-demo.html#filter-select-and-create-features",
    "href": "notebooks/baseline-model/baseline-model-demo.html#filter-select-and-create-features",
    "title": "Using the linked dataset in a baseline model for predicting disease outbreak periods",
    "section": "Filter, select, and create features",
    "text": "Filter, select, and create features\n\n# add labels to data\n\n\nto remove this part after we get generated dataset from linked notebook\n\n\n1. Filter to year with existing OSM and population data\nOSM-related features are only available from year 2014 onwards and the population features are only availble until year 2020.\n\nlinked_data = linked_data[(linked_data[\"year\"] &gt;= 2014) & (linked_data[\"year\"] &lt;= 2020)]\n\n\nlinked_data.shape\n\n(364, 254)\n\n\n\nprint(linked_data.columns.tolist())\n\n['Date', 'Source', 'year', 'Month', 'Week', 'Region', 'PSGC_Region', 'Municipality', 'ADM3_PCODE', 'ICD', 'Disease', 'Cases', 'Claims', 'Deaths', 'Case_Type', 'Date_Type', 'outbreak', 'outbreak_group', 'CO_AVG_mean', 'CO_MIN_mean', 'CO_MAX_mean', 'CO_STD_mean', 'WEIGHTED_AVG_CO_mean', 'HI_AVG_mean', 'HI_MIN_mean', 'HI_MAX_mean', 'HI_STD_mean', 'WEIGHTED_AVG_HI_mean', 'NDVI_AVG_mean', 'NDVI_MIN_mean', 'NDVI_MAX_mean', 'NDVI_STD_mean', 'WEIGHTED_AVG_NDVI_mean', 'NO2_AVG_mean', 'NO2_MIN_mean', 'NO2_MAX_mean', 'NO2_STD_mean', 'WEIGHTED_AVG_NO2_mean', 'O3_AVG_mean', 'O3_MIN_mean', 'O3_MAX_mean', 'O3_STD_mean', 'WEIGHTED_AVG_O3_mean', 'PM10_AVG_mean', 'PM10_MIN_mean', 'PM10_MAX_mean', 'PM10_STD_mean', 'WEIGHTED_AVG_PM10_mean', 'PM25_AVG_mean', 'PM25_MIN_mean', 'PM25_MAX_mean', 'PM25_STD_mean', 'WEIGHTED_AVG_PM25_mean', 'PNP_AVG_mean', 'PNP_MIN_mean', 'PNP_MAX_mean', 'PNP_STD_mean', 'WEIGHTED_AVG_PNP_mean', 'PR_AVG_mean', 'PR_MIN_mean', 'PR_MAX_mean', 'PR_STD_mean', 'WEIGHTED_AVG_PR_mean', 'RH_AVG_mean', 'RH_MIN_mean', 'RH_MAX_mean', 'RH_STD_mean', 'WEIGHTED_AVG_RH_mean', 'SO2_AVG_mean', 'SO2_MIN_mean', 'SO2_MAX_mean', 'SO2_STD_mean', 'WEIGHTED_AVG_SO2_mean', 'SPI3_AVG_mean', 'SPI3_MIN_mean', 'SPI3_MAX_mean', 'SPI3_STD_mean', 'WEIGHTED_AVG_SPI3_mean', 'SPI6_AVG_mean', 'SPI6_MIN_mean', 'SPI6_MAX_mean', 'SPI6_STD_mean', 'WEIGHTED_AVG_SPI6_mean', 'SR_AVG_mean', 'SR_MIN_mean', 'SR_MAX_mean', 'SR_STD_mean', 'WEIGHTED_AVG_SR_mean', 'Tave_AVG_mean', 'Tave_MIN_mean', 'Tave_MAX_mean', 'Tave_STD_mean', 'WEIGHTED_AVG_Tave_mean', 'Tmax_AVG_mean', 'Tmax_MIN_mean', 'Tmax_MAX_mean', 'Tmax_STD_mean', 'WEIGHTED_AVG_Tmax_mean', 'Tmin_AVG_mean', 'Tmin_MIN_mean', 'Tmin_MAX_mean', 'Tmin_STD_mean', 'WEIGHTED_AVG_Tmin_mean', 'UVR_AVG_mean', 'UVR_MIN_mean', 'UVR_MAX_mean', 'UVR_STD_mean', 'WEIGHTED_AVG_UVR_mean', 'WS_AVG_mean', 'WS_MIN_mean', 'WS_MAX_mean', 'WS_STD_mean', 'WEIGHTED_AVG_WS_mean', 'poi_count_sum', 'poi_count_mean', 'clinic_count_sum', 'clinic_count_mean', 'dentist_count_sum', 'dentist_count_mean', 'doctors_count_sum', 'doctors_count_mean', 'hospital_count_sum', 'hospital_count_mean', 'optician_count_sum', 'optician_count_mean', 'pharmacy_count_sum', 'pharmacy_count_mean', 'drinking_water_count_sum', 'drinking_water_count_mean', 'water_mill_count_sum', 'water_mill_count_mean', 'water_tower_count_sum', 'water_tower_count_mean', 'water_works_count_sum', 'water_works_count_mean', 'water_well_count_sum', 'water_well_count_mean', 'sanitary_dump_station_count_sum', 'sanitary_dump_station_count_mean', 'toilet_count_sum', 'toilet_count_mean', 'recycling_count_sum', 'recycling_count_mean', 'waste_basket_count_sum', 'waste_basket_count_mean', 'wastewater_plant_count_sum', 'wastewater_plant_count_mean', 'waste_transfer_station_count_sum', 'waste_transfer_station_count_mean', 'weighted_avg_clinic_nearest_mean', 'weighted_avg_dentist_nearest_mean', 'weighted_avg_doctors_nearest_mean', 'weighted_avg_hospital_nearest_mean', 'weighted_avg_optician_nearest_mean', 'weighted_avg_pharmacy_nearest_mean', 'weighted_avg_drinking_water_nearest_mean', 'weighted_avg_water_mill_nearest_mean', 'weighted_avg_water_tower_nearest_mean', 'weighted_avg_water_works_nearest_mean', 'weighted_avg_water_well_nearest_mean', 'weighted_avg_sanitary_dump_station_nearest_mean', 'weighted_avg_toilet_nearest_mean', 'weighted_avg_recycling_nearest_mean', 'weighted_avg_waste_basket_nearest_mean', 'weighted_avg_wastewater_plant_nearest_mean', 'weighted_avg_waste_transfer_station_nearest_mean', 'weighted_avg_osm_river_nearest_mean', 'weighted_avg_osm_stream_nearest_mean', 'weighted_avg_osm_canal_nearest_mean', 'weighted_avg_osm_drain_nearest_mean', 'weighted_avg_osm_wetland_nearest_mean', 'weighted_avg_osm_reservoir_nearest_mean', 'weighted_avg_osm_water_nearest_mean', 'weighted_avg_osm_riverbank_nearest_mean', 'weighted_avg_osm_dock_nearest_mean', 'pop_count_total', 'pop_density_per_m2', 'brgy_pop_count_mean', 'brgy_total_area_mean', 'avg_rad_min_mean', 'avg_rad_max_mean', 'avg_rad_mean_mean', 'avg_rad_std_mean', 'avg_rad_median_mean', 'doh_pois_count_sum', 'doh_pois_count_mean', 'doh_brgy_health_station_count_sum', 'doh_brgy_health_station_count_mean', 'doh_rural_health_unit_count_sum', 'doh_rural_health_unit_count_mean', 'doh_hospital_count_sum', 'doh_hospital_count_mean', 'doh_birthing_home_lying_in_clinic_count_sum', 'doh_birthing_home_lying_in_clinic_count_mean', 'doh_infirmary_count_sum', 'doh_infirmary_count_mean', 'doh_drug_abuse_treatment_rehabilitation_center_count_sum', 'doh_drug_abuse_treatment_rehabilitation_center_count_mean', 'doh_social_hygiene_clinic_count_sum', 'doh_social_hygiene_clinic_count_mean', 'doh_medical_clinic_count_sum', 'doh_medical_clinic_count_mean', 'weighted_avg_doh_brgy_health_station_nearest_mean', 'weighted_avg_doh_rural_health_unit_nearest_mean', 'weighted_avg_doh_hospital_nearest_mean', 'weighted_avg_doh_birthing_home_lying_in_clinic_nearest_mean', 'weighted_avg_doh_infirmary_nearest_mean', 'weighted_avg_doh_drug_abuse_treatment_rehabilitation_center_nearest_mean', 'weighted_avg_doh_social_hygiene_clinic_nearest_mean', 'weighted_avg_doh_medical_clinic_nearest_mean', 'pct_area_flood_hazard_100yr_low_mean', 'pct_area_flood_hazard_100yr_med_mean', 'pct_area_flood_hazard_100yr_high_mean', 'pct_area_flood_hazard_25yr_low_mean', 'pct_area_flood_hazard_25yr_med_mean', 'pct_area_flood_hazard_25yr_high_mean', 'pct_area_flood_hazard_5yr_low_mean', 'pct_area_flood_hazard_5yr_med_mean', 'pct_area_flood_hazard_5yr_high_mean', 'pct_area_landslide_hazard_low_mean', 'pct_area_landslide_hazard_med_mean', 'pct_area_landslide_hazard_high_mean', 'pct_area_bare_sparse_vegetation_mean', 'pct_area_builtup_mean', 'pct_area_cropland_mean', 'pct_area_grassland_mean', 'pct_area_herbaceous_wetland_mean', 'pct_area_mangroves_mean', 'pct_area_permanent_water_bodies_mean', 'pct_area_shrubland_mean', 'pct_area_tree_cover_mean', 'google_bldgs_count_sum', 'google_bldgs_count_mean', 'google_bldgs_area_total_sum', 'google_bldgs_area_total_mean', 'google_bldgs_area_mean_sum', 'google_bldgs_area_mean_mean', 'google_bldgs_count_lt100_sqm_sum', 'google_bldgs_count_lt100_sqm_mean', 'google_bldgs_count_100_200_sqm_sum', 'google_bldgs_count_100_200_sqm_mean', 'google_bldgs_count_gt_200_sqm_sum', 'google_bldgs_count_gt_200_sqm_mean', 'google_bldgs_density_sum', 'google_bldgs_density_mean', 'google_bldgs_pct_built_up_area_sum', 'google_bldgs_pct_built_up_area_mean', 'travel_time', 'hospital_pop_reached_total', 'hospital_pct_population_reached', 'healthcenter_pop_reached_total', 'healthcenter_pct_population_reached', 'rhu_pop_reached_total', 'rhu_pct_population_reached']\n\n\n\n\n2. Inspect missing data\n\nprint(linked_data.columns[linked_data.isnull().any()].tolist())\n\n['Claims', 'Deaths', 'PNP_AVG_mean', 'PNP_MIN_mean', 'PNP_MAX_mean', 'PNP_STD_mean', 'WEIGHTED_AVG_PNP_mean', 'SPI3_AVG_mean', 'SPI3_MIN_mean', 'SPI3_MAX_mean', 'SPI3_STD_mean', 'WEIGHTED_AVG_SPI3_mean', 'SPI6_AVG_mean', 'SPI6_MIN_mean', 'SPI6_MAX_mean', 'SPI6_STD_mean', 'WEIGHTED_AVG_SPI6_mean']\n\n\n\nto remove this part after we get generated dataset from linked notebook\n\n\n\n3. Remove columns that could cause the model confusion\n\n# remove ookla for now\nfiltered_linked_df = linked_data.drop(\n    columns=[\n        \"Source\",\n        \"year\",\n        \"Month\",\n        \"Week\",\n        \"Region\",\n        \"PSGC_Region\",\n        \"Municipality\",\n        \"ICD\",\n        \"Disease\",\n        \"Claims\",\n        \"Deaths\",\n        \"Case_Type\",\n        \"Date_Type\",\n        \"outbreak_group\",\n        # remove climate features with nulls\n        \"PNP_AVG_mean\",\n        \"PNP_MIN_mean\",\n        \"PNP_MAX_mean\",\n        \"PNP_STD_mean\",\n        \"WEIGHTED_AVG_PNP_mean\",\n        \"SPI3_AVG_mean\",\n        \"SPI3_MIN_mean\",\n        \"SPI3_MAX_mean\",\n        \"SPI3_STD_mean\",\n        \"WEIGHTED_AVG_SPI3_mean\",\n        \"SPI6_AVG_mean\",\n        \"SPI6_MIN_mean\",\n        \"SPI6_MAX_mean\",\n        \"SPI6_STD_mean\",\n        \"WEIGHTED_AVG_SPI6_mean\",\n        # remove static columns\n        # health facility POIs that are static\n        \"doh_pois_count_sum\",\n        \"doh_pois_count_mean\",\n        \"doh_brgy_health_station_count_sum\",\n        \"doh_brgy_health_station_count_mean\",\n        \"doh_rural_health_unit_count_sum\",\n        \"doh_rural_health_unit_count_mean\",\n        \"doh_hospital_count_sum\",\n        \"doh_hospital_count_mean\",\n        \"doh_birthing_home_lying_in_clinic_count_sum\",\n        \"doh_birthing_home_lying_in_clinic_count_mean\",\n        \"doh_infirmary_count_sum\",\n        \"doh_infirmary_count_mean\",\n        \"doh_drug_abuse_treatment_rehabilitation_center_count_sum\",\n        \"doh_drug_abuse_treatment_rehabilitation_center_count_mean\",\n        \"doh_social_hygiene_clinic_count_sum\",\n        \"doh_social_hygiene_clinic_count_mean\",\n        \"doh_medical_clinic_count_sum\",\n        \"doh_medical_clinic_count_mean\",\n        \"weighted_avg_doh_brgy_health_station_nearest_mean\",\n        \"weighted_avg_doh_rural_health_unit_nearest_mean\",\n        \"weighted_avg_doh_hospital_nearest_mean\",\n        \"weighted_avg_doh_birthing_home_lying_in_clinic_nearest_mean\",\n        \"weighted_avg_doh_infirmary_nearest_mean\",\n        \"weighted_avg_doh_drug_abuse_treatment_rehabilitation_center_nearest_mean\",\n        \"weighted_avg_doh_social_hygiene_clinic_nearest_mean\",\n        \"weighted_avg_doh_medical_clinic_nearest_mean\",\n        # static flood hazard features\n        \"pct_area_flood_hazard_100yr_low_mean\",\n        \"pct_area_flood_hazard_100yr_med_mean\",\n        \"pct_area_flood_hazard_100yr_high_mean\",\n        \"pct_area_flood_hazard_25yr_low_mean\",\n        \"pct_area_flood_hazard_25yr_med_mean\",\n        \"pct_area_flood_hazard_25yr_high_mean\",\n        \"pct_area_flood_hazard_5yr_low_mean\",\n        \"pct_area_flood_hazard_5yr_med_mean\",\n        \"pct_area_flood_hazard_5yr_high_mean\",\n        \"pct_area_landslide_hazard_low_mean\",\n        \"pct_area_landslide_hazard_med_mean\",\n        \"pct_area_landslide_hazard_high_mean\",\n        # static landcover features\n        \"pct_area_bare_sparse_vegetation_mean\",\n        \"pct_area_builtup_mean\",\n        \"pct_area_cropland_mean\",\n        \"pct_area_grassland_mean\",\n        \"pct_area_herbaceous_wetland_mean\",\n        \"pct_area_mangroves_mean\",\n        \"pct_area_permanent_water_bodies_mean\",\n        \"pct_area_shrubland_mean\",\n        \"pct_area_tree_cover_mean\",\n        # static infrastructure features\n        \"google_bldgs_count_sum\",\n        \"google_bldgs_count_mean\",\n        \"google_bldgs_area_total_sum\",\n        \"google_bldgs_area_total_mean\",\n        \"google_bldgs_area_mean_sum\",\n        \"google_bldgs_area_mean_mean\",\n        \"google_bldgs_count_lt100_sqm_sum\",\n        \"google_bldgs_count_lt100_sqm_mean\",\n        \"google_bldgs_count_100_200_sqm_sum\",\n        \"google_bldgs_count_100_200_sqm_mean\",\n        \"google_bldgs_count_gt_200_sqm_sum\",\n        \"google_bldgs_count_gt_200_sqm_mean\",\n        \"google_bldgs_density_sum\",\n        \"google_bldgs_density_mean\",\n        \"google_bldgs_pct_built_up_area_sum\",\n        \"google_bldgs_pct_built_up_area_mean\",\n        # static isochrone features\n        \"travel_time\",\n        \"hospital_pop_reached_total\",\n        \"hospital_pct_population_reached\",\n        \"healthcenter_pop_reached_total\",\n        \"healthcenter_pct_population_reached\",\n        \"rhu_pop_reached_total\",\n        \"rhu_pct_population_reached\",\n    ]\n)\n\n\nto remove this part after we get generated dataset from linked notebook\n\n\n\n4. Handle rows with null values\n\nReplace null values for Nightlight features with 0\n\n\nntl_features = [\n    \"avg_rad_min_mean\",\n    \"avg_rad_max_mean\",\n    \"avg_rad_mean_mean\",\n    \"avg_rad_std_mean\",\n    \"avg_rad_median_mean\",\n]\nfiltered_linked_df[ntl_features] = filtered_linked_df[ntl_features].fillna(0)\n\n\n\n5. Create outbreak label\nWe will now create another column indicating if that week is an outbreak week or not based on the number of cases. This is an important prerequisite step for our modeling. For this demonstration, we define an outbreak as follows:\n\nThe start of an outbreak occurs when the week’s cases equal to or higher than a threshold.\n\nThe threshold is defined as the 75th percentile of weekly cases based on years when a substantial number of cases are recorded.\n\nThe end of an outbreak occurs during the week when cases fall below the threshold for at least 2 weeks.\n\nWe use this definition to create column outbreak containing 1 if the week is classified as an outbreak, and 0 if its a non-outbreak.\n\n## Add code here\n\n# get substantial years with high number of cases recorded\nagg_year = linked_data.groupby([\"Year\"])[\"Cases\"].sum().reset_index(name=\"Cases\")\nagg_year.linked_data(\"Cases\", ascending=False)\n\nAfter getting the years with the highest number of cases recorded, we will temporarily subset the data and get the distribution of the throughout those years only. Use the .describe() function to see the percentiles.\n\ntop_years_list = [\n    2011.0,\n    2012.0,\n    2013.0,\n    2017.0,\n    2018.0,\n    2019.0,\n    2020.0,\n    2021.0,\n    2022.0,\n]\nsubset_zamb = linked_data[linked_data[\"Year\"].isin(top_years_list)]\nsubset_zamb[\"Cases\"].describe()\n\nUpdate the OUTBREAK_THRESHOLD to serve as the marker whether a week will be tagged as an outbreak or not.\n\n# Edit this value to your identified threshold\n# Set your threshold value\nOUTBREAK_THRESHOLD = 44\n\nAfter running the step below, you should be able to see an additional column outbreak at the rightmost part of the table containing 0s and 1s. This means that the health data has been labeled and now prepared for modeling.\n\n# Create a new column 'outbreak' initialized with zeros\nlinked_data[\"outbreak\"] = 0\n\n# Track weeks satisfying the threshold, and count consecutive outbreak weeks\nconsecutive_weeks_below_threshold = 0\nprevious_outbreak = False\n\nfor index, row in linked_data.iterrows():\n    if row[\"Cases\"] &gt; OUTBREAK_THRESHOLD:\n        consecutive_weeks_below_threshold = 0\n        previous_outbreak = True\n        linked_data.at[index, \"outbreak\"] = 1\n    else:\n        # check if previous week was an outbreak\n        # if yes, continue to tag current week as outbreak\n        # part of observation period\n        if previous_outbreak and consecutive_weeks_below_threshold &lt; 2:\n            linked_data.at[index, \"outbreak\"] = 1\n            consecutive_weeks_below_threshold += 1\n        elif consecutive_weeks_below_threshold == 2:\n            linked_data.at[index, \"outbreak\"] = 0\n            previous_outbreak = False\n        else:\n            linked_data.at[index, \"outbreak\"] = 0\n\n\n# view data\nlinked_data.head()\n\n\n\n5. Create lagged features for Cases\nAdding previous week’s cases as features may help enhance the model’s capability in predicting future cases.\nYou can try experimenting with the lagged features by adding or removing them.\n\n# Group by barangay and return the date of the previous number of cases\nfiltered_linked_df = filtered_linked_df.assign(\n    prev_1_wk_numcases=filtered_linked_df[\"Cases\"].shift(\n        1, fill_value=0\n    ),  # cases a week before\n    prev_2_wk_numcases=filtered_linked_df[\"Cases\"].shift(\n        2, fill_value=0\n    ),  # cases 2 weeks before\n    prev_3_wk_numcases=filtered_linked_df[\"Cases\"].shift(\n        3, fill_value=0\n    ),  # cases 3 weeks before\n    prev_1_mo_numcases=filtered_linked_df[\"Cases\"].shift(\n        4, fill_value=0\n    ),  # cases 1 month before\n)\nfiltered_linked_df.shape\n\n(364, 159)\n\n\n\n# print out final columns\nprint(list(filtered_linked_df.columns))\n\n['Date', 'ADM3_PCODE', 'Cases', 'outbreak', 'CO_AVG_mean', 'CO_MIN_mean', 'CO_MAX_mean', 'CO_STD_mean', 'WEIGHTED_AVG_CO_mean', 'HI_AVG_mean', 'HI_MIN_mean', 'HI_MAX_mean', 'HI_STD_mean', 'WEIGHTED_AVG_HI_mean', 'NDVI_AVG_mean', 'NDVI_MIN_mean', 'NDVI_MAX_mean', 'NDVI_STD_mean', 'WEIGHTED_AVG_NDVI_mean', 'NO2_AVG_mean', 'NO2_MIN_mean', 'NO2_MAX_mean', 'NO2_STD_mean', 'WEIGHTED_AVG_NO2_mean', 'O3_AVG_mean', 'O3_MIN_mean', 'O3_MAX_mean', 'O3_STD_mean', 'WEIGHTED_AVG_O3_mean', 'PM10_AVG_mean', 'PM10_MIN_mean', 'PM10_MAX_mean', 'PM10_STD_mean', 'WEIGHTED_AVG_PM10_mean', 'PM25_AVG_mean', 'PM25_MIN_mean', 'PM25_MAX_mean', 'PM25_STD_mean', 'WEIGHTED_AVG_PM25_mean', 'PR_AVG_mean', 'PR_MIN_mean', 'PR_MAX_mean', 'PR_STD_mean', 'WEIGHTED_AVG_PR_mean', 'RH_AVG_mean', 'RH_MIN_mean', 'RH_MAX_mean', 'RH_STD_mean', 'WEIGHTED_AVG_RH_mean', 'SO2_AVG_mean', 'SO2_MIN_mean', 'SO2_MAX_mean', 'SO2_STD_mean', 'WEIGHTED_AVG_SO2_mean', 'SR_AVG_mean', 'SR_MIN_mean', 'SR_MAX_mean', 'SR_STD_mean', 'WEIGHTED_AVG_SR_mean', 'Tave_AVG_mean', 'Tave_MIN_mean', 'Tave_MAX_mean', 'Tave_STD_mean', 'WEIGHTED_AVG_Tave_mean', 'Tmax_AVG_mean', 'Tmax_MIN_mean', 'Tmax_MAX_mean', 'Tmax_STD_mean', 'WEIGHTED_AVG_Tmax_mean', 'Tmin_AVG_mean', 'Tmin_MIN_mean', 'Tmin_MAX_mean', 'Tmin_STD_mean', 'WEIGHTED_AVG_Tmin_mean', 'UVR_AVG_mean', 'UVR_MIN_mean', 'UVR_MAX_mean', 'UVR_STD_mean', 'WEIGHTED_AVG_UVR_mean', 'WS_AVG_mean', 'WS_MIN_mean', 'WS_MAX_mean', 'WS_STD_mean', 'WEIGHTED_AVG_WS_mean', 'poi_count_sum', 'poi_count_mean', 'clinic_count_sum', 'clinic_count_mean', 'dentist_count_sum', 'dentist_count_mean', 'doctors_count_sum', 'doctors_count_mean', 'hospital_count_sum', 'hospital_count_mean', 'optician_count_sum', 'optician_count_mean', 'pharmacy_count_sum', 'pharmacy_count_mean', 'drinking_water_count_sum', 'drinking_water_count_mean', 'water_mill_count_sum', 'water_mill_count_mean', 'water_tower_count_sum', 'water_tower_count_mean', 'water_works_count_sum', 'water_works_count_mean', 'water_well_count_sum', 'water_well_count_mean', 'sanitary_dump_station_count_sum', 'sanitary_dump_station_count_mean', 'toilet_count_sum', 'toilet_count_mean', 'recycling_count_sum', 'recycling_count_mean', 'waste_basket_count_sum', 'waste_basket_count_mean', 'wastewater_plant_count_sum', 'wastewater_plant_count_mean', 'waste_transfer_station_count_sum', 'waste_transfer_station_count_mean', 'weighted_avg_clinic_nearest_mean', 'weighted_avg_dentist_nearest_mean', 'weighted_avg_doctors_nearest_mean', 'weighted_avg_hospital_nearest_mean', 'weighted_avg_optician_nearest_mean', 'weighted_avg_pharmacy_nearest_mean', 'weighted_avg_drinking_water_nearest_mean', 'weighted_avg_water_mill_nearest_mean', 'weighted_avg_water_tower_nearest_mean', 'weighted_avg_water_works_nearest_mean', 'weighted_avg_water_well_nearest_mean', 'weighted_avg_sanitary_dump_station_nearest_mean', 'weighted_avg_toilet_nearest_mean', 'weighted_avg_recycling_nearest_mean', 'weighted_avg_waste_basket_nearest_mean', 'weighted_avg_wastewater_plant_nearest_mean', 'weighted_avg_waste_transfer_station_nearest_mean', 'weighted_avg_osm_river_nearest_mean', 'weighted_avg_osm_stream_nearest_mean', 'weighted_avg_osm_canal_nearest_mean', 'weighted_avg_osm_drain_nearest_mean', 'weighted_avg_osm_wetland_nearest_mean', 'weighted_avg_osm_reservoir_nearest_mean', 'weighted_avg_osm_water_nearest_mean', 'weighted_avg_osm_riverbank_nearest_mean', 'weighted_avg_osm_dock_nearest_mean', 'pop_count_total', 'pop_density_per_m2', 'brgy_pop_count_mean', 'brgy_total_area_mean', 'avg_rad_min_mean', 'avg_rad_max_mean', 'avg_rad_mean_mean', 'avg_rad_std_mean', 'avg_rad_median_mean', 'prev_1_wk_numcases', 'prev_2_wk_numcases', 'prev_3_wk_numcases', 'prev_1_mo_numcases']\n\n\n\nfiltered_linked_df.head()\n\n\n\n\n\n\n\n\n\nDate\nADM3_PCODE\nCases\noutbreak\nCO_AVG_mean\nCO_MIN_mean\nCO_MAX_mean\nCO_STD_mean\nWEIGHTED_AVG_CO_mean\nHI_AVG_mean\n...\nbrgy_total_area_mean\navg_rad_min_mean\navg_rad_max_mean\navg_rad_mean_mean\navg_rad_std_mean\navg_rad_median_mean\nprev_1_wk_numcases\nprev_2_wk_numcases\nprev_3_wk_numcases\nprev_1_mo_numcases\n\n\n\n\n312\n2014-01-06\nPH097332000\n0.0\n0\n0.071684\n0.061561\n0.081937\n0.007905\n0.071684\n27.839929\n...\n1.515541e+07\n1.738257\n5.266028\n2.946094\n1.036254\n2.805252\n0.0\n0.0\n0.0\n0.0\n\n\n313\n2014-01-06\nPH097332000\n0.0\n0\n0.071684\n0.061561\n0.081937\n0.007905\n0.071684\n27.839929\n...\n1.515541e+07\n1.738257\n5.266028\n2.946094\n1.036254\n2.805252\n0.0\n0.0\n0.0\n0.0\n\n\n314\n2014-01-13\nPH097332000\n0.0\n0\n0.080858\n0.069232\n0.093571\n0.008738\n0.080858\n26.511542\n...\n1.515541e+07\n1.738257\n5.266028\n2.946094\n1.036254\n2.805252\n0.0\n0.0\n0.0\n0.0\n\n\n315\n2014-01-20\nPH097332000\n0.0\n0\n0.081604\n0.075696\n0.091231\n0.005616\n0.081604\n26.305700\n...\n1.515541e+07\n1.738257\n5.266028\n2.946094\n1.036254\n2.805252\n0.0\n0.0\n0.0\n0.0\n\n\n316\n2014-01-27\nPH097332000\n0.0\n0\n0.074839\n0.071346\n0.081812\n0.004384\n0.074839\n26.145516\n...\n1.515541e+07\n1.738257\n5.266028\n2.946094\n1.036254\n2.805252\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 159 columns"
  },
  {
    "objectID": "notebooks/baseline-model/baseline-model-demo.html#setup-model-training",
    "href": "notebooks/baseline-model/baseline-model-demo.html#setup-model-training",
    "title": "Using the linked dataset in a baseline model for predicting disease outbreak periods",
    "section": "Setup model training",
    "text": "Setup model training\n\n1. Split to train-test set\nWe use TimeSeriesSplit to properly cross-validate the model.\n\nfiltered_linked_df = filtered_linked_df.set_index(\"Date\")\n\n\n# split features and target label\nX = filtered_linked_df.drop(labels=[\"outbreak\"], axis=1)\ny = filtered_linked_df[\"outbreak\"]\n\n\ntss = TimeSeriesSplit(n_splits=3)\nfor train_index, test_index in tss.split(X):\n    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n\n# set these aside to link back later to the predictions for analysis\nbrgy_tests = X_test[\"ADM3_PCODE\"]\nnumcases_test = X_test[\"Cases\"]\n\n# drop the columns above\nX_train = X_train.drop(columns=[\"ADM3_PCODE\", \"Cases\"])\nX_test = X_test.drop(columns=[\"ADM3_PCODE\", \"Cases\"])\n\n\n\n2. Check train and test split\nCheck the exact lengths of the train-test split. For our use case, having a test set consisting of a year and 6 months is enough. This may be changed depending on the disease chosen.\nYou may experiment the splits by changing the n_splits parameter in the TimeSeriesSplit step above. Increasing the n_splits would give a narrower time period for the test set.\n\nlen(X_train), len(X_test)\n\n(273, 91)\n\n\n\n# training data date coverage\nX_train.index.min(), X_train.index.max()\n\n(Timestamp('2014-01-06 00:00:00'), Timestamp('2019-03-25 00:00:00'))\n\n\n\n# testing data date coverage\nX_test.index.min(), X_test.index.max()\n\n(Timestamp('2019-04-01 00:00:00'), Timestamp('2020-12-21 00:00:00'))"
  },
  {
    "objectID": "notebooks/baseline-model/baseline-model-demo.html#train-a-randomforest-classifier",
    "href": "notebooks/baseline-model/baseline-model-demo.html#train-a-randomforest-classifier",
    "title": "Using the linked dataset in a baseline model for predicting disease outbreak periods",
    "section": "Train a RandomForest Classifier",
    "text": "Train a RandomForest Classifier\n\n1. Optimize model parameters using GridSearchCV\n\n# Define model and parameter space to optimize\nclassifier = RandomForestClassifier(random_state=42)\n\nparam_grid = {\n    \"n_estimators\": [50, 100, 200, 300, 500],\n    \"max_depth\": [None, 5, 10, 15, 20],\n}\n\n\nmodel_grid_search = GridSearchCV(classifier, param_grid, verbose=1, cv=5)\nmodel_grid_search.fit(X_train, y_train)\n\nFitting 5 folds for each of 25 candidates, totalling 125 fits\n\n\nGridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42),\n             param_grid={'max_depth': [None, 5, 10, 15, 20],\n                         'n_estimators': [50, 100, 200, 300, 500]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42),\n             param_grid={'max_depth': [None, 5, 10, 15, 20],\n                         'n_estimators': [50, 100, 200, 300, 500]},\n             verbose=1)estimator: RandomForestClassifierRandomForestClassifier(random_state=42)RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\nprint(f\"Best hyperparameters: {model_grid_search.best_params_}\")\nprint(f\"Train best score: {model_grid_search.best_score_:0.4}\")\nprint(\n    f\"Train best score std: {np.std(model_grid_search.cv_results_['mean_test_score']):0.4}\"\n)\n\nBest hyperparameters: {'max_depth': None, 'n_estimators': 50}\nTrain best score: 0.8706\nTrain best score std: 0.003837\n\n\n\n\n2. Check model performance\nApply the model with the best parameters to the test set to check its performance. The outputted score is the mean accuracy.\n\ntest_score = model_grid_search.score(X_test, y_test)\nprint(f\"Test best score: {test_score:0.6}\")\n\nTest best score: 0.791209\n\n\n\n# Evaluate the model on test data\nbest_rf_classifier_model = model_grid_search.best_estimator_\ntest_accuracy = best_rf_classifier_model.score(X_test, y_test)\nprint(\"Test Accuracy:\", test_accuracy)\n\nTest Accuracy: 0.7912087912087912\n\n\nThe best model predicts outbreaks with 79.12% accuracy"
  },
  {
    "objectID": "notebooks/baseline-model/baseline-model-demo.html#inspect-model-results",
    "href": "notebooks/baseline-model/baseline-model-demo.html#inspect-model-results",
    "title": "Using the linked dataset in a baseline model for predicting disease outbreak periods",
    "section": "Inspect model results",
    "text": "Inspect model results\n\n1. Make predictions on the test set\nNow that we have chosen the best model to use, we can now run the best RandomForest Classifier model on our test set and extract the predicted values.\nTo get the discrete outbreak class prediction, outbreak (1) or not (0), run .predict. In this demonstration, we’d also like to include the predicted probabilities that comes along with the class predictions using the predict_proba function for analysis.\n\ny_pred = best_rf_classifier_model.predict(X_test)\ny_pred_proba = best_rf_classifier_model.predict_proba(X_test)\npred_results = pd.DataFrame(y_pred)\npred_results.describe()\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\ncount\n91.000000\n\n\nmean\n0.252747\n\n\nstd\n0.436995\n\n\nmin\n0.000000\n\n\n25%\n0.000000\n\n\n50%\n0.000000\n\n\n75%\n0.500000\n\n\nmax\n1.000000\n\n\n\n\n\n\n\n\n\n\n2. Visualize results and metrics\nReview the results of the RF classification predictions using the classification_report. This summarizes on one table the accuracy, precision, recall, f1-score, and support (number of instances per class).\nIndicated in the classification report a mean accuracy of 79.12% is achieved by the selected best model. Taking a look at other metrics, the model is able to correctly predict no outbreak weeks (0) more than outbreak weeks (1) with a recall of 81.82% compared to predictions of outbreak with a recall of 64.39%.\nImportant to also look at the support column. Support simply refers to the number of instances that we have in our test set for each outbreak label. In our test set, we have 77 weeks that have no outbreaks and 14 weeks that have outbreaks.\n\nreport = classification_report(y_test, y_pred, output_dict=True)\nreport_df = pd.DataFrame(report).transpose()\nreport_df\n\n\n\n\n\n\n\n\n\nprecision\nrecall\nf1-score\nsupport\n\n\n\n\n0\n0.926471\n0.818182\n0.868966\n77.000000\n\n\n1\n0.391304\n0.642857\n0.486486\n14.000000\n\n\naccuracy\n0.791209\n0.791209\n0.791209\n0.791209\n\n\nmacro avg\n0.658887\n0.730519\n0.677726\n91.000000\n\n\nweighted avg\n0.844137\n0.791209\n0.810123\n91.000000\n\n\n\n\n\n\n\n\nTo view how many of our predictions are correct against the actual values, we can use a confusion matrix. After running the cell below, you should be able to see a 2x2 matrix of true labels and predicted labels.\n\nBased on the confusion matrix, we were able to correctly predict 63 no outbreak weeks out of 77 and for the outbreak weeks we were only able to predict 9 out of those 14 weeks from the test set. The model still experiences certain confusion when it comes to predicting for outbreak weeks since there are 5 weeks that were mislabeled as no outbreak (False Negatives) which could indicate a limitation of our dataset.\n\ncm = confusion_matrix(y_test, y_pred, labels=best_rf_classifier_model.classes_)\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm, display_labels=best_rf_classifier_model.classes_\n)\ndisp.plot()\nplt.show()\n\n\n\n\n\n\n\n\nTo understand analyze the results of the model more, let’s visualize the predicted probabilities from the model through a colored histogram of the values.\nIn the plot below, the predicted probabilities are on the x-axis and the y-axis is the number of those instances. The colors are based on the actual labels. The expectation is the bars colored as blue should skew to the left side and the red bars skew to the right indicating that the model is able to distinguish the presence of an outbreak based on the features we’ve given.\nMost of the outbreak weeks range from the 0.4 to 0.6 probabilities which indicates that the model finds difficulty in strongly distinguishing weeks with a dengue outbreak.\n\nprediction = y_pred_proba[:, 1]  # prediction for outbreak\nplt.figure(figsize=(15, 7))\nplt.hist(prediction[y_test == 0], bins=50, label=\"No Outbreak\")\nplt.hist(prediction[y_test == 1], bins=50, label=\"Outbreak\", alpha=0.7, color=\"r\")\nplt.xlabel(\"Probability of being Outbreak Class\", fontsize=10)\nplt.ylabel(\"Number of records in each bucket\", fontsize=10)\nplt.legend(fontsize=15, title=\"Actual Labels\")\nplt.tick_params(axis=\"both\", labelsize=8, pad=5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3. Identify features that best explains outbreak prediction\n\nexplainer = shap.Explainer(best_rf_classifier_model)\nshap_values = explainer.shap_values(X_test)\n\nFrom the SHAP, what impacted the predictions the most are the cases from the previous weeks, nighttime lights radiance, population, and distances to health facilities.\nIt is interesting to observe that the nighttime lights features and presence of POIs impact the outbreak prediction. This indicates that how urbanized a city has a possible influence on determining an outbreak.\n\nshap.summary_plot(shap_values[0], X_test)"
  },
  {
    "objectID": "notebooks/baseline-model/baseline-model-demo.html#analyzing-outbreak-predictions",
    "href": "notebooks/baseline-model/baseline-model-demo.html#analyzing-outbreak-predictions",
    "title": "Using the linked dataset in a baseline model for predicting disease outbreak periods",
    "section": "Analyzing outbreak predictions",
    "text": "Analyzing outbreak predictions\nPrepare the dataframe to include the actual number of cases and predictions on one table.\n\n# Join back the predictions to the features for analysis.\nanalyze_df = X_test.copy()\nanalyze_df[\"ADM3_PCODE\"] = brgy_tests\nanalyze_df[\"Cases\"] = numcases_test\nanalyze_df[\"actual_class\"] = y_test\nanalyze_df[\"predicted_class\"] = y_pred\nanalyze_df[\"predicted_proba_outbreak\"] = y_pred_proba[:, 1]\nanalyze_df = analyze_df.reset_index()\n\n\nanalyize_df.head(2)\n\n\n1. Visualizing Actual vs. Predicted Outbreaks Periods\n\nactual_outbreaks_summary = create_outbreak_summary(analyze_df, \"actual_class\")\nactual_outbreaks_summary[\"category\"] = \"actual\"\npredicted_outbreaks_summary = create_outbreak_summary(analyze_df, \"predicted_class\")\npredicted_outbreaks_summary[\"category\"] = \"predicted\"\n\n\noutbreak_lengths_results = pd.concat(\n    [actual_outbreaks_summary, predicted_outbreaks_summary]\n)\noutbreak_lengths_results\n\n\n\n\n\n\n\n\n\noutbreak_group\nstart_date\nend_date\nactual_length_weeks\ncategory\n\n\n\n\n0\n1\n2019-04-22\n2019-05-06\n3\nactual\n\n\n1\n3\n2019-08-19\n2019-09-16\n5\nactual\n\n\n2\n5\n2019-10-21\n2019-11-04\n3\nactual\n\n\n3\n7\n2019-11-25\n2019-12-09\n3\nactual\n\n\n0\n1\n2019-04-29\n2019-04-29\n1\npredicted\n\n\n1\n3\n2019-06-17\n2019-06-24\n2\npredicted\n\n\n2\n5\n2019-07-08\n2019-07-08\n1\npredicted\n\n\n3\n7\n2019-07-29\n2019-07-29\n1\npredicted\n\n\n4\n9\n2019-08-19\n2019-09-09\n4\npredicted\n\n\n5\n11\n2019-10-28\n2019-11-04\n2\npredicted\n\n\n6\n13\n2019-12-02\n2019-12-16\n3\npredicted\n\n\n7\n15\n2020-01-06\n2020-01-06\n2\npredicted\n\n\n8\n17\n2020-01-27\n2020-02-03\n2\npredicted\n\n\n9\n19\n2020-02-17\n2020-02-24\n2\npredicted\n\n\n10\n21\n2020-06-29\n2020-06-29\n1\npredicted\n\n\n11\n23\n2020-08-17\n2020-08-17\n1\npredicted\n\n\n12\n25\n2020-10-19\n2020-10-19\n1\npredicted\n\n\n\n\n\n\n\n\nRunning the cell below should produce a dumbbell plot that shows the actual outbreak periods from the health data versus the predicted outbreak periods.\nThe resulting plot should look similar to this. The plot illustrates the outbreak periods from the dataframe prepared above.   \nThis is only a sample plot, the resulting plot will not be exactly the same dependeing on the range of the test set.\n\n# Set custom colors\nclass_colors = {\"predicted\": \"#ee472f\", \"actual\": \"#53bed0\"}\n\n# Plot start and end outbreak points\nfig = px.scatter(\n    outbreak_lengths_results,\n    x=[\"start_date\", \"end_date\"],\n    y=\"category\",\n    color=\"category\",\n    size=\"actual_length_weeks\",\n    labels={\"x\": \"Date\"},\n    category_orders={\"category\": [\"actual\", \"predicted\"]},\n    color_discrete_map=class_colors,\n)\n\nfor i, row in outbreak_lengths_results.iterrows():\n    fig.add_trace(\n        go.Scatter(\n            mode=\"lines\",\n            # layer=\"below\",\n            line=dict(\n                color=class_colors[row[\"category\"]],\n                width=3,\n                dash=\"solid\",\n            ),\n            # connect the two markers\n            x=[row[\"start_date\"], row[\"end_date\"]],\n            y=[row[\"category\"], row[\"category\"]],\n            showlegend=False,\n        )\n    )\n\n# Update layout with title and axis titles\nfig.update_layout(\n    title=\"Dengue Outbreak Periods for Zamboanga City\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"Category\",\n)\nfig.show()\n\n                                                \n\n\nThe actual outbreak periods are only 4 in total, however, the predicted outbreak periods are 13. Many of which look to be one week outbreak periods only.\nThe outbreak periods predicted in 2019 coincides with the actual outbreak periods by only being off by a week or two. The one week outbreaks during 2019 also occur near the actual date of the outbreak. Meanwhile, for 2020 there should have been no outbreaks for that year, however, sparse 1-2 week outbreak periods were predicted. It is possible that training on the year 2019 where the dengue cases peaked the most could have influenced the model into continuously predicting outbreaks for 2020 in single weeks that have a high number of cases.\n\n\n2. Visualize actual Number of Cases with Predicted label\nLet’s take a look now at the actual number of cases for the test set and what was the predicted outbreak label for them are.\n\nfig, ax = plt.subplots(figsize=(16, 6))\n\n# Define colors for each class\nclass_colors = {0: \"#53bed0\", 1: \"#ee472f\"}\n\n# Plot the bar plot\nax.bar(\n    analyze_df[\"Date\"],\n    analyze_df[\"Cases\"],\n    alpha=0.5,\n    label=\"Bar Plot\",\n    width=8,\n    color=[class_colors[c] for c in analyze_df[\"predicted_class\"]],\n)\n\n# Rotate x-axis labels\nplt.xticks(rotation=30)\n\n# Create legend handles and labels for bar plot\nlegend_handles = [\n    plt.Rectangle((0, 0), 1, 1, color=color) for color in class_colors.values()\n]\nlegend_labels = list(class_colors.keys())\n\n# Add legend for bar plot\nax.legend(legend_handles, legend_labels, fontsize=\"large\", title=\"Predicted Outbreak\")\n# Add title and axis labels\nax.set_title(\"Dengue Outbreak Periods for Brgy. Tetuan in Zamboanga\")\nax.set_xlabel(\"Date\")\nax.set_ylabel(\"Number of Cases\")\nplt.show()\n\n\n\n\n\n\n\n\nThe model was able to determine outbreaks for the weeks in 2019 and 2020 with the highest number of cases for those years. For the rest of 2020, the number of cases are consistently low, however, for the weeks that seem to have a slight increase were labeled as an outbreak.\nTo understand this more, let’s try to plot the probabilities along with the number of cases through a time series plot.\n\nplt.figure(figsize=(10, 6))\n\n# Plotting the line plot for actual cases with a different color\nplt.plot(\n    analyze_df[\"Date\"],\n    analyze_df[\"Cases\"],\n    color=\"#ff5733\",  # Reddish color for better visibility\n    label=\"Actual Cases\",\n)\n\n# Define your own gradient color\ngradient_color = \"#53bed0\"  # Blueish color for the gradient\n\n# Define the number of points for the gradient\nnum_points = len(analyze_df)\n\n# Create a custom colormap with the gradient color\ncolors = [(0, gradient_color), (1, gradient_color)]\ncmap = LinearSegmentedColormap.from_list(\"custom_gradient\", colors)\n\n# Creating a background gradient for predicted probabilities\nfor i in range(len(analyze_df) - 1):\n    start_date = analyze_df[\"Date\"].iloc[i]\n    end_date = analyze_df[\"Date\"].iloc[i + 1]\n    alpha = analyze_df[\"predicted_proba_outbreak\"].iloc[\n        i\n    ]  # Use predicted probability as alpha\n    plt.fill_between(\n        analyze_df[\"Date\"],  # x-values (dates)\n        analyze_df[\"Cases\"].min(),  # Bottom y-value (minimum of actual cases)\n        analyze_df[\"Cases\"].max(),  # Top y-value (maximum of actual cases)\n        where=(\n            (analyze_df[\"Date\"] &gt;= start_date) & (analyze_df[\"Date\"] &lt;= end_date)\n        ),  # Condition for filling the area\n        color=cmap(i / num_points),  # Use colormap to set gradient color\n        alpha=alpha,  # Set alpha based on predicted probability\n        label=\"Predicted Probabilities\"\n        if i == 0\n        else None,  # Include legend only for the first fill_between\n    )\n\n# Add legend for the line plot and the gradient\nplt.legend()\n\n# Setting labels and title\nplt.xlabel(\"Time\")\nplt.ylabel(\"Cases\")\nplt.title(\"Actual Cases with Predicted Probabilities\")\n\n# Displaying the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIn this plot, the line plot represents the number of cases over time and the gradient in the background represents the predicted probablity of an outbreak for that week. The darker the color the more likely it is to be classified as an outbreak.\nWeeks with sudden spikes are able to identified by the model, howeever, these are not exact. For sudden spikes, the model seems to consider the weeks after to be the outbreak."
  }
]